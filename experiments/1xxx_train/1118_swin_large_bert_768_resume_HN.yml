batch_size: 16
amp: True
max_grad_norm: 4
preprocessed_dir: ${env:INPUTDIR}/kfujikawa/bms-preprocess-hn-v1
lr: 1e-5
epoch: 10
in_chans: 1
image_size_h: 768
image_size_w: 768
accum: 2
ratio: 0.2

image_transforms_train:
  class: AlbumentationCompose
  transforms:
    - class: Binalize
    - class: SaltAndPepperNoise
      p: 0.5
    - class: Denoise
      kernel_size: 5
      p: 0.5
    - class: Resize
      height: ${image_size_h}
      width: ${image_size_w}
    - class: Flip
      p: 0.5
    - class: ToTensorV2

image_transforms_infer:
  class: AlbumentationCompose
  transforms:
    - class: Binalize
    - class: Denoise
      kernel_size: 5
    - class: Resize
      height: ${image_size_h}
      width: ${image_size_w}
    - class: ToTensorV2

based_on:
  - /work/experiments/1xxx_train/1100_image-captioning-with-padding-base.yml

model_params:
  class: ImageCaptioningModel
  tokenizer: ${tokenizer_params}
  encoder:
    class: PretrainedSwinTransformer
    model_name: swin_base_patch4_window12_384_in22k
    url: /work/output/1106_swin_bert_384/fold=0/best.pth
    num_classes: 0
    in_chans: ${in_chans}
    img_size:
      - ${image_size_h}
      - ${image_size_w}
  decoder:
    class: AutoTransformerDecoder
    model_name: bert
    is_decoder: True
    num_hidden_layers: 3
    add_cross_attention: True
    use_cache: False
stages:
  data_params:
    loaders_params:
      train:
        batch_size: 1
        shuffle:
        drop_last:
        batch_sampler_params:
          class: BMSBalanceClassSampler
          class_name: is_HN
          batch_size: ${batch_size}
          ratio: ${ratio}
  scheduler_params:
    class: CosineAnnealingLR
    T_max: 10
    eta_min: 1e-6
  callbacks_params:
    loss:
      class: NNCompBatchMetricCallback
      prefix: loss
      metric:
        class: CrossEntropyLoss2D
        ignore_index: 0
      input_key: next_token_ids
      output_key: logits
    resume:
      class: SwinTransformerResumeTrainingCallback
      checkpoint_dir: /work/output/1106_swin_bert_384
